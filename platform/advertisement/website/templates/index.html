{% extends "base.html" %} {% block title %}Radiology{% endblock %} {% block content
%}
<section id="section_main">
    <h1 class="main_title">Radiology</h1>
</section>
 
<section  id="section_mission_statement">
    <h2 class="section_title">Product Mission Statement</h2>
    <p>
        Despite significant advances in artificial intelligence (AI) for medical image analysis, any researcher seeking to implement AI in radiology faces significant hurdles. 
        Familiarity with programming languages and machine learning is required to apply AI models to medical images and evaluate the model’s performance. 
        If the researcher wishes to correct their model’s predictions to improve its accuracy, this requires additional effort. 
        This semester’s effort is a continuation of Spring 2024’s CMSC435 Radiology project, consisting of Nina Johe, Aryan Kakadia, Muzzamil Khan, Morgan Ko, Josh Leeman, Maximilian Son, and Sashwat Venkatesh. 
        This team integrated the ability to make model predictions into an existing medical image viewer. 
        The proceeding issues with the workflow are what this semester’s group seeks to fix: after obtaining the model predictions, the researcher must manually display the results. 
        To manually edit the predicted masks for model improvement, the researcher must use a medical image viewer that supports it, or implement it themselves. 
        Determining model accuracy requires writing a separate program, meaning additional technical knowledge is needed. In other words, the current workflow, although improved by last year’s efforts, still requires several disparate programs, leading to more points of failure. 

    </p>
    <p>
        This project’s previous contributors had delivered a product capable of storing and viewing medical images, as well as sending these images to machine learning models on the cloud to make predictions in a single click. 
        These predictions are in the form of masks, which are layers over a medical image that represent a model’s classification of each pixel. 
        This solution leveraged OHIF, an open-sourced medical image viewer; Orthanc, an open-sourced server for storing medical images; and Google Cloud for model storage and predictions. 
        Our group intends to extend the functionality of this product to allow radiology researchers to evaluate and improve model performance. 
        After receiving a model’s prediction masks, the user will be able to display them with the click of a button. 
        If ground truth masks are available, the accuracy of a model’s predictions will be made visible without additional input required from the user. 
        In addition, users will be able to manually adjust these prediction masks, and then save these edited masks for use in active learning. 
        Detailed descriptions of these functionalities will be explained in later sections. 
        Although existing software allows for each of these tasks to be completed separately, our product will combine these features into a centralized workflow, making the application of machine learning techniques on medical images more straightforward. 
        Many tasks will be able to be accomplished with a single click. 
        As such, each step of this workflow will not require extensive technical knowledge, greatly simplifying the process of applying AI to medical images for researchers. 

    </p>
</section>

<section id="section_core_features">
    <h2 class="section_title">Core Features and Functionalities</h2>
    <p>Prediction Mask Editing for Active Learning:</p>
    <p>After the user requests a prediction from a model, they will have their medical images returned with multiple colored outlines known as masks that each represent whatever the machine learning model detected. 
        Each outline is a layer that can be displayed or hidden. 
        If a “ground truth mask” is already provided in the DICOM file (these are premade outlines for parts of the image the AI model is meant to detect), an accuracy score (DICE score) is provided. 
        This ground truth mask can also be displayed or hidden by the user. 
        If a ground truth mask is not in the file, users will have an editing tool that allows them to shape an outline around the actual parts of the image they want to have detected. 
        These edited masks can then be saved. These masks are intended to be used for active learning, but model training is out of our scope. As such, we will only implement the ability to export these masks.</p>

    <p>DICE Score:</p>
    <p>In the above section, it is mentioned that an accuracy score will be given for prediction masks. 
        The measurement we will use is known as a DICE score. This score represents the similarity of two data sets, with the data sets in this case being mask outlines. 
        Our main reason for using this metric is that it is heavily used in medical imaging and computer science. This makes it the most relevant metric for any professional in the medical field.</p>

    <p>These features are demonstrated through the following screenshots and videos:</p>
    <!-- Add screenshots or video embeds here -->
</section>

<section id="section_devs_and_credits" class="devs_and_credits">
    <h2 class="section_title">Developers and Credits</h2>
    <p>
        This project was developed by a dedicated team of students. The team members and their respective roles are as follows:
    </p>
    <table class="devs_and_credits_table" width="100%">
        <tr>
            <td>
                <img class="headshot" src="{{ url_for('static', filename='assets/images/temp.jpg') }}" alt="Team Member 1 Image">
                <p>Amanuel Seifu</p>
                <p>Role and Responsibilities:</p>
                <p>SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING</p>
            </td>
            <td>
                <img class="headshot" src="{{ url_for('static', filename='assets/images/dsyomichev_headshot.jpg') }}" alt="Team Member 2 Image">
                <p>Daniel Syomichev</p>
                <p>Role and Responsibilities:</p>
                <p>SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING</p>
            </td>
            <td>
                <img class="headshot" src="{{ url_for('static', filename='assets/images/temp.jpg') }}" alt="Team Member 3 Image">
                <p>Eric Chang</p>
                <p>Role and Responsibilities:</p>
                <p>SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING</p>
            </td>
        </tr>
        <tr>
            <td>
                <img class="headshot" src="{{ url_for('static', filename='assets/images/temp.jpg') }}" alt="Team Member 4 Image">
                <p>Guang-Lin Wei</p>
                <p>Role and Responsibilities:</p>
                <p>SOMETHING SOMETHING SOMETHING SOMETHIN</p>
            </td>
            <td>
                <img class="headshot" src="{{ url_for('static', filename='assets/images/temp.jpg') }}" alt="Team Member 5 Image">
                <p>Ian Gordon</p>
                <p>Role and Responsibilities:</p>
                <p>SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING</p>
            </td>
            <td>
                <img class="headshot" src="{{ url_for('static', filename='assets/images/temp.jpg') }}" alt="Team Member 6 Image">
                <p>Padmini Gopinath</p>
                <p>Role and Responsibilities:</p>
                <p>SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING SOMETHING</p>
            </td>
        </tr>
    </table>
    <p>
        We also acknowledge the contributions of the previous year's team for their foundational work on this project.
    </p>
    <ul>
        <li>Nina Johe</li>
        <li>Aryan Kakadia</li>
        <li>Muzzamil Khan</li>
        <li>Morgan Ko</li>
        <li>Josh Leeman</li>
        <li>Maximilian Son</li>
        <li>Sashwat Venkatesh</li>
    </ul>
    <p>
        We would also like to thank our clients and our professor for guiding us through the development of this
        product.
    </p>
    <ul>
        <li>Neehar Peri</li>
        <li>Rahul Pemmaraju</li>
        <li>Dr. James Purtilo</li>
    </ul>
</section>

<section id="section_user_doc">
    <h2 class="section_title">User Documentation</h2>
    <p>Follow these step-by-step instructions to use the application:</p>
    <ul>
        <li>How to upload a DICOM image in OHIF.</li>
        <li>How to run a prediction on an uploaded DICOM image and interpret returned DICE scores.</li>
        <li>How to use basic OHIF features such as annotation/markup tools to edit segmentation masks.</li>
        <li>How to save these edited segmentation masks.</li>
    </ul>
    <h2 class="section_title">Installation Guide</h2>
    <p>
        To install the application on your local machine, follow these steps:
    </p>
    <ol>
        <li>STEP 1</li>
        <li>STEP 2</li>
        <li>STEP 3</li>
        <li>STEP 4</li>
    </ol>
</section>

<section id="section_dev_doc">
    <h2 class="section_title">Developer Documentation</h2>
    <p>SOME STEPS TO SET UP REPOSITORY AND ALL DEPENDENCIES ETC...</p>
</section>

<section id="section_contact">
    <h2 class="section_title">Contact Us</h2>
    <p>
        If you have any questions or feedback, please reach out to us at:
    </p>
    <ul>
        <li>Email: </li>
        <li>Phone: </li>
    </ul>
</section>

{% endblock %}